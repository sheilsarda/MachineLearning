{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HMM.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ULGOSdt96Jhb"
      },
      "source": [
        "# **CIS 520: Machine Learning, Fall 2020**\n",
        "# **Week 11, Worksheet 3**\n",
        "## **Hidden Markov Models**\n",
        "\n",
        "\n",
        "- **Content Creator:** Yide Zhao\n",
        "- **Content Checkers:** Gautam Ramesh, Yang Yan"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C7DB6t-l6R-0"
      },
      "source": [
        "**Hidden Markov Models (HMM)**\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v_Ccd6zq6UEU"
      },
      "source": [
        "Let's work through an example of an HMM to see how probability propagates and to find the hidden states.\n",
        "\n",
        "\n",
        "Example: On any given day, Alice is in one of two states: happy or sad. You do not know her internal state, but get to observe her activities in the evening. Each evening, she either sings, goes for a walk, or watches TV.\n",
        "\n",
        "Alice's state on any day is random. Her state $Z_{1}$ on day 1 is equally likely to be happy or sad:\n",
        "$$\n",
        "P\\left(Z_{1}=\\text { happy }\\right)=1 / 2\n",
        "$$\n",
        "Given her state $Z_{t}$ on day $t,$ her state $Z_{t+1}$ on the next day is governed by the following probabilities (and is Markovian: conditionally independent of her previous states and activities):\n",
        "$$\n",
        "P\\left(Z_{t+1}=\\text { happy } \\mid Z_{t}=\\text { happy }\\right)=4 / 5 \\quad P\\left(Z_{t+1}=\\text { happy } \\mid Z_{t}=\\mathrm{sad}\\right)=1 / 2\n",
        "$$\n",
        "Alice's activities are also random. Her activities vary based on her state; given her state $Z_{t}$ on day $t,$ her activity $X_{t}$ on that day is governed by the following probabilities (and is conditionally independent of everything else $)$\n",
        "$$\n",
        "\\begin{array}{ll}\n",
        "P\\left(X_{t}=\\operatorname{sing} \\mid Z_{t}=\\text { happy }\\right)=5 / 10 & P\\left(X_{t}=\\operatorname{sing} \\mid Z_{t}=\\mathrm{sad}\\right)=1 / 10 \\\\\n",
        "P\\left(X_{t}=\\text { walk } \\mid Z_{t}=\\text { happy }\\right)=3 / 10 & P\\left(X_{t}=\\text { walk } \\mid Z_{t}=\\mathrm{sad}\\right)=2 / 10 \\\\\n",
        "P\\left(X_{t}=\\mathrm{TV} \\mid Z_{t}=\\text { happy }\\right)=2 / 10 & P\\left(X_{t}=\\mathrm{TV} \\mid Z_{t}=\\mathrm{sad}\\right)=7 / 10\n",
        "\\end{array}\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vHizl_dv6W5x"
      },
      "source": [
        "Let's now calculate the joint probability of \n",
        "$$\n",
        "\\begin{array}{l}\n",
        "P\\left(X_{1: 2}=(\\operatorname{sing}, \\mathrm{TV}), Z_{1: 2}=(\\text { happy, happy })\\right) \\\\\n",
        "P\\left(X_{1: 2}=(\\operatorname{sing}, \\mathrm{TV}), Z_{1: 2}=(\\text { happy }, \\mathrm{sad})\\right)\n",
        "\\end{array}\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QamLHc5p6ZGo"
      },
      "source": [
        "The probability of z1, the first hidden state, being happy is 1/2. Given the z1 is happy, z2, the second hidden state, being happy is 4/5. Lastly, given z1 = happy, the probability of sing is 5/10. Given z2 = happy, the probability of TV is 2/10. This give us the following formula.\n",
        "$$\n",
        "P\\left(X_{1: 2}=(\\operatorname{sing}, T V), Z_{1: 2}=(\\text {happy,happy})\\right)=\\frac{1}{2} *\\left(\\frac{4}{5}\\right) *\\left(\\frac{5}{10} \\cdot \\frac{2}{10}\\right)=\\frac{1}{25}=0.04\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wUYQ4Ier6c-1"
      },
      "source": [
        "## Exercise: \n",
        "Calculate $P\\left(X_{1: 2}=(\\mathrm{sing}, \\mathrm{TV}), Z_{1: 2}=(\\text { happy }, \\mathrm{sad})\\right)$. Show the step in #TODO to get the answer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "meffH72-6dNs"
      },
      "source": [
        "$P\\left(X_{1: 2}=(\\mathrm{sing}, \\mathrm{TV}), Z_{1: 2}=(\\text { happy }, \\mathrm{sad})\\right) = \\text{#TODO} = 0.035$  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k3mHDPN1lvNU"
      },
      "source": [
        "**Markov Models and their steady state**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vrb9SBnciAuj"
      },
      "source": [
        "Let's look  at the properties of a Markov transition matrix, and in particular what it will  converge to at steady state.  \n",
        "Markov Matrices are square, but not symmetric, which means you need to be a little careful when computing eigenvectors (they have both left and right ones)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AZdBS6HV-9Ks",
        "outputId": "c2f74deb-fcfd-4fb5-dafa-c79f330eb1e6"
      },
      "source": [
        " # confirm that what the Markov sequence converges to\n",
        " import numpy  as np\n",
        " A = np.array([[0.8, 0.2], [0.6, 0.4]])\n",
        " s = np.array([0, 1])\n",
        " s1 = np.array([0.3, 0.7])  # starting point doesn't matter\n",
        "print(s@A@A@A@A@A@A)\n",
        "print(s1@A@A@A@A@A@A)\n",
        "print([0.75, 0.25]@A)\n",
        "print(A.T @np.array([0.75, 0.25]).T)\n",
        "print('eigenvectors')\n",
        "np.linalg.eig(A) # biggest eigenvalue is always 1."
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.749952 0.250048]\n",
            "[0.7499712 0.2500288]\n",
            "[0.75 0.25]\n",
            "[0.75 0.25]\n",
            "eigenvectors\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([1. , 0.2]), array([[ 0.70710678, -0.31622777],\n",
              "        [ 0.70710678,  0.9486833 ]]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nmt7py3DDUfe",
        "outputId": "434a0294-5648-4d8e-b8d3-00c771b9adf4"
      },
      "source": [
        "# The transpose is closer to how we usually write things; but (again) \n",
        "# non-symmetric matrices don't give us the nice orthagonality we expect\n",
        "print(\"reversed:\",A.T@A.T@A.T@A.T@A.T@A.T@s.T)\n",
        "# which of  the following are eigenvectors of A transpose?\n",
        "print(A.T @ np.array([0.94868, 0.31622]).T)\n",
        "print(A.T @ np.array([0.75, 0.25]).T)\n",
        "print(A.T @ np.array([-0.707107, 0.707107]).T /0.2)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "reversed: [0.749952 0.250048]\n",
            "[0.948676 0.316224]\n",
            "[0.75 0.25]\n",
            "[-0.707107  0.707107]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7tF74pA3YnQh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6d7d011d-34fb-4b4d-a019-d9b0f6955331"
      },
      "source": [
        "# but if we're careful we can figure out  what repeated matrix multiplications\n",
        "# will give us\n",
        "(eig, eigv) = np.linalg.eig(A.T)\n",
        "print('eigenvalues', eig)\n",
        "print('left eigenvectors', eigv)\n",
        "#A.T @A.T @A.T @A.T @A.T @A.T @ s.T\n",
        "first_eigv = eigv[:,0]\n",
        "print('Eigenvectors are normalized using L2 norm, but we want to find a probability,')\n",
        "print('which is L1-normalized')\n",
        "print('first eigenvector, rescaled:', first_eigv/np.sum(first_eigv))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "eigenvalues [1.  0.2]\n",
            "left eigenvectors [[ 0.9486833  -0.70710678]\n",
            " [ 0.31622777  0.70710678]]\n",
            "Eigenvectors are normalized using L2 norm, but we want to find a probability,\n",
            "which is L1-normalized\n",
            "first eigenvector, rescaled: [0.75 0.25]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}