{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Ensemble Method.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"Q5cwDBcWYQEM","colab_type":"text"},"source":["# Ensemble Methods"]},{"cell_type":"code","metadata":{"id":"ZZs0phvfH4k7","colab_type":"code","colab":{}},"source":["from sklearn.datasets import fetch_california_housing\n","X, y = fetch_california_housing(return_X_y=True)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hm6HbukzN2jW","colab_type":"text"},"source":["Ensemble methods combine multiple models to obtain better predictive performance than could be obtained from any of the constituent models alone. Specifically in this notebook, we will look at how to use several decision trees classifiers to produce better predictive performance than a single decision tree classifier. \n","\n"]},{"cell_type":"markdown","metadata":{"id":"9-LDhV2aNMZ2","colab_type":"text"},"source":["# Gradient Tree Boosting - Boosting"]},{"cell_type":"markdown","metadata":{"id":"nTwgnqsWhAO7","colab_type":"text"},"source":["Boosting is used to create a collection of predictors. In this technique, learners are learned sequentially with early learners fitting simple models to the data and then analysing data for errors. Consecutive trees (random sample) are fit and at every step, the goal is to improve the accuracy from the prior tree. When an input is misclassified by a hypothesis, its weight is increased so that next hypothesis is more likely to classify it correctly. This process converts weak learners into better performing model. [1] Specifically, the following graph shows the process of gradient tree boosting: each tree (weak learner) are added **sequentially** to the model to create better model. \n","<p align=\"center\">\n","<img src=\"https://github.com/yidezhao/cis520/blob/master/boost.png?raw=true\" width = \"400\"/>\n","<p/>\n","Now, this notebook will guide you through the boosting process.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"98cUaMjtj-KP","colab_type":"text"},"source":["The algorithm looks as follows [2]:\n","<p align=\"center\">\n","<img src=\"https://github.com/yidezhao/cis520/blob/master/boostalgo.png?raw=true\" width = \"800\"/>\n","<p/>\n","For the loss function, we will use the least square function $$Loss Function = \\frac{1}{2}(y-f(x))^2$$. "]},{"cell_type":"markdown","metadata":{"id":"Vx1p04NwCz65","colab_type":"text"},"source":["Step 1: Given the loss function above, $F_0(x)$ will be the average of the label. \n","\n","Step 2: In the for loop, we add weak learners sequentially to our model. "]},{"cell_type":"markdown","metadata":{"id":"_674wQiIDtOO","colab_type":"text"},"source":["# Exercise 1: \n","What is the equation for the pseudo-residual $r_{im}$ in step 2.1 from our loss function? Is this the actual residual?"]},{"cell_type":"markdown","metadata":{"id":"kcK3H1IyD1yZ","colab_type":"text"},"source":["Answer:  $r_{im} = -(y-f(x))$"]},{"cell_type":"code","metadata":{"id":"evKfJSjOIKWi","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vNw4_ludEMyh","colab_type":"text"},"source":["Step 2.2: Build a decision tree to fit the pseudo-residual for all the data points. The pseudo-residual is the new label that we will use to build the decision tree. Note: we run on all n points in the  dataset.\n","\n","Step 2.3: Calculate the number of observations classified into each leaf of the decision tree.\n","\n","Step 2.4: Update the model by adding the (learning rate * weak learner) to the it. Graphically, this is what we see in first graph under Gradient Boosting.\n","\n","Step 3: Output the model."]},{"cell_type":"code","metadata":{"id":"B3XWhOmeNYV2","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1597714301012,"user_tz":240,"elapsed":2214,"user":{"displayName":"Yide Zhao","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiuGC8NFVIB9UrjAiP6aPzUpqLyGoWLHZWfGOF2=s64","userId":"03443959525138035544"}},"outputId":"a3da6aea-6068-4aa2-d69a-d08d164bffbe"},"source":["from sklearn.datasets import make_regression\n","from sklearn.ensemble import GradientBoostingRegressor\n","from sklearn.model_selection import train_test_split\n","X, y = make_regression(n_samples=2000, random_state=0)\n","X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n","\n","model = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, max_depth=1, random_state=0, loss='ls')\n","model.fit(X_train, y_train)\n","model.score(X_test, y_test)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.7577277689079992"]},"metadata":{"tags":[]},"execution_count":24}]},{"cell_type":"markdown","metadata":{"id":"CtCDIkQrDP1Y","colab_type":"text"},"source":["# Exercise 2:\n","1. Change the max_depth from 1 to 3. Does this improve your accuracy? Why or why not?\n","2. Change the n_estimators from 100 to 200. Does this improve your accuracy? Why or why not?"]},{"cell_type":"markdown","metadata":{"id":"JMwk0cH2H3iM","colab_type":"text"},"source":["Answer: "]},{"cell_type":"markdown","metadata":{"id":"uSwrv0uKJP12","colab_type":"text"},"source":["Now, let try to use Gradient Boost to do classification. In classification, you will use the exact same steps as above to calculate a probability (between 0 and 1). For details, you're welcome to watch this vidio: https://www.youtube.com/watch?v=jxuNLH5dXCs"]},{"cell_type":"code","metadata":{"id":"CYdaRVQsJOUv","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1597714071677,"user_tz":240,"elapsed":781,"user":{"displayName":"Yide Zhao","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiuGC8NFVIB9UrjAiP6aPzUpqLyGoWLHZWfGOF2=s64","userId":"03443959525138035544"}},"outputId":"b52446cd-c63e-4b56-8b09-46a8a039fcc7"},"source":["from sklearn.datasets import make_classification\n","from sklearn.ensemble import GradientBoostingClassifier\n","from sklearn.model_selection import train_test_split\n","X, y = make_classification(n_samples=2000, random_state=0)\n","X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n","\n","model = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, max_depth=1, random_state=0)\n","model.fit(X_train, y_train)\n","model.score(X_test, y_test)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.978"]},"metadata":{"tags":[]},"execution_count":16}]},{"cell_type":"markdown","metadata":{"id":"100xBpFONYxE","colab_type":"text"},"source":["# Random Forest - Bagging\n","\n","Bagging creates many subsets of the training sample, by sampling randomly with replacement. Each collection of subset data is used to train a decision tree. The predictions from the resulting ensemble of trees are then voted or averaged. The results is more robust than a single decision tree classifier.\n","\n","The stepss are straightforward. \\\\\n","1) Bootstrap a subset of data from the training data. \\\\\n","2) build a single decision tree over the data. \\\\\n","3) repeat 1 and 2 until you get enough trees. \\\\\n","4) Take a majority vote from all your decision trees. \\\\\n","Graphically, it looks like the following:\n","<p align=\"center\">\n","<img src=\"https://github.com/yidezhao/cis520/blob/master/bag.png?raw=true\" width = \"400\"/>\n","<p/>"]},{"cell_type":"code","metadata":{"id":"5N6f9lv3NrlI","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1597714274688,"user_tz":240,"elapsed":3977,"user":{"displayName":"Yide Zhao","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiuGC8NFVIB9UrjAiP6aPzUpqLyGoWLHZWfGOF2=s64","userId":"03443959525138035544"}},"outputId":"6699dd8f-b3bb-47e0-aa2e-22b267c21ab5"},"source":["from sklearn.datasets import make_regression\n","from sklearn.ensemble import RandomForestRegressor\n","from sklearn.model_selection import train_test_split\n","X, y = make_regression(n_samples=2000, random_state=0)\n","X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n","\n","model = RandomForestRegressor(n_estimators=100, max_depth=4, random_state=0, bootstrap = True)\n","model.fit(X_train, y_train)\n","model.score(X_test, y_test)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.600484459925607"]},"metadata":{"tags":[]},"execution_count":21}]},{"cell_type":"markdown","metadata":{"id":"SGgKvq0iMayR","colab_type":"text"},"source":["Now, let's try to make some classifications."]},{"cell_type":"code","metadata":{"id":"bYfbUm4-Mfls","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1597714374497,"user_tz":240,"elapsed":828,"user":{"displayName":"Yide Zhao","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiuGC8NFVIB9UrjAiP6aPzUpqLyGoWLHZWfGOF2=s64","userId":"03443959525138035544"}},"outputId":"80932a6f-b682-494a-d5b3-6fa0ca659979"},"source":["from sklearn.datasets import make_classification\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.model_selection import train_test_split\n","X, y = make_classification(n_samples=2000, random_state=0)\n","X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n","\n","model = RandomForestClassifier(n_estimators=100, max_depth=4, random_state=0, bootstrap = True)\n","model.fit(X_train, y_train)\n","model.score(X_test, y_test)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.98"]},"metadata":{"tags":[]},"execution_count":26}]},{"cell_type":"markdown","metadata":{"id":"SbSAEMDwMtNG","colab_type":"text"},"source":["# Exercise 3:\n","Is each of the following statements True or False? If False, why? \n","1. Both Random Forest and Gradient Boost are stochastic in that different runs on the same dataset will yield different resulst.\n","2. Random Forest can be easily run in parallel, since each tree does not depend on previous trees.\n","3. Gradient Boost can be easily run in parallel, since each tree does not depend on previous trees."]},{"cell_type":"markdown","metadata":{"id":"FbuLDpxzNNI1","colab_type":"text"},"source":["Answer: \\\\\n","1. False. Gradient Boost is deterministic and will yield the same result on different run.\n","2. True.\n","3. False. In Gradient Boost algorithm, we calculate the pseudo-residual based on all the previous trees which are included in the model."]},{"cell_type":"markdown","metadata":{"id":"HOPc4E_1h78X","colab_type":"text"},"source":["Reference:\n","\n","[1] Bagging and Boosting: https://analyticsindiamag.com/primer-ensemble-learning-bagging-boosting \\\\\n","[2] Boosting algorithm: https://en.wikipedia.org/wiki/Gradient_boosting\n"]}]}