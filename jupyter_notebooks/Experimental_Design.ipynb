{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Experimental_Design.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"cq4alx7BcTiK","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":139},"outputId":"e7f213ae-833a-4299-a4c6-f499b8a6c49c"},"source":["#Assume we have an initial data set (X,y) \n","# and want to know whether to label a new point x1 or x2\n","import numpy as np \n","X = np.random.random_sample((5,3))\n","print(X)\n","y = X @ np.array([1.0,1.0,1.0]) + np.random.normal(0,0.25,len(X))\n","print('y:',y)\n","w = np.linalg.inv(X.T@X)@ X.T@y\n","print('w:',w)\n","x1 = np.random.random_sample((1,3))\n","x2 = np.random.random_sample((1,3))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[[0.58160247 0.08239375 0.94080134]\n"," [0.94777716 0.40859616 0.61581903]\n"," [0.8239238  0.7931139  0.63265291]\n"," [0.99197352 0.28222987 0.02458456]\n"," [0.31504212 0.59148018 0.99573592]]\n","y: [1.42552021 2.39198928 2.33319205 1.06008647 1.53724875]\n","w: [1.11623725 1.01058632 0.82192974]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"QbafOBrEe9Ax","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":52},"outputId":"b0346661-4d43-4f5e-8479-f03e2a61fb07"},"source":["# We want to minimize the variance of our estimate of our parameter estimate of w\n","# so pick the new point to label which will minimize (X'X)^{-1}\n","X1 = np.vstack([X,x1])\n","X2 = np.vstack([X,x2])\n","\n","# Compute the Trace, for A optimal\n","X1A = np.trace(np.linalg.inv(X1.T@X1))\n","X2A = np.trace(np.linalg.inv(X2.T@X2))\n","print(\"Trace norms from X1 and X2\", X1A, X2A)\n","# you will get different results each time, but pick the new x that \n","# gave the smaller value\n","\n","\n","\n","# instead or the log determinant, for D optimal\n","X1D = np.log(np.linalg.det(np.linalg.inv(X1.T@X1)))\n","X2D = np.log(np.linalg.det(np.linalg.inv(X2.T@X2)))\n","\n","print(\"log determinant norms from X1 and X2\", X1D, X2D)\n","\n","\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Trace norms from X1 and X2 4.405754320432505 3.7714438358907585\n","log determinant norms from X1 and X2 -0.5427493140017275 -0.7416018411622476\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"CcjVgAUvhXjZ","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":104},"outputId":"cb77719d-f5bf-44f8-8863-6c0d060a084f"},"source":["# an aside on matrix norms:\n","# For any symmetric and positive-definite matrix A:\n","#    Frobenius norm of A = [trace(AA')]^1/2\n","\n","A = np.linalg.inv(X1.T@X1)\n","print(A)\n","print(\"Frobenious norm\", np.linalg.norm(A))\n","print(\"from the trace\", np.sqrt(np.trace(A@A.T)))\n","\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[[ 0.93636999 -0.84475036 -0.29342892]\n"," [-0.84475036  2.61245702 -0.62682121]\n"," [-0.29342892 -0.62682121  0.85692731]]\n","Frobenious norm 3.289568652115702\n","from the trace 3.289568652115702\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"xlFjWGD7s_Ld","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":212},"outputId":"8da1eb50-596e-4890-f1dc-4fd4288523d5"},"source":["# Now, let's see which prediction is \"most uncertain\" for active learning\n","# we'll turn to a classification problem\n","from sklearn.linear_model import LogisticRegression\n","ys = np.array([0,0,1,1,1]).T\n","print(X)\n","print(ys)\n","logreg = LogisticRegression(random_state=0).fit(X,ys)\n","print('p(x1) ', logreg.predict_proba(x1)[0][0])\n","print('p(x2) ', logreg.predict_proba(x2)[0][0])\n","# the \"most uncertain x\" is the one that gives a prediction closest to 0.5\n","# that is the one we want to label"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[[0.58160247 0.08239375 0.94080134]\n"," [0.94777716 0.40859616 0.61581903]\n"," [0.8239238  0.7931139  0.63265291]\n"," [0.99197352 0.28222987 0.02458456]\n"," [0.31504212 0.59148018 0.99573592]]\n","[0 0 1 1 1]\n","p(x1)  0.47487652367026756\n","p(x2)  0.40403338143166057\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n","  FutureWarning)\n"],"name":"stderr"}]}]}