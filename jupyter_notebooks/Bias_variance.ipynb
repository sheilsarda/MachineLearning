{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Bias_variance.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.10"}},"cells":[{"cell_type":"markdown","metadata":{"colab_type":"text","id":"YRK7gnp-C8pz"},"source":["# **CIS 520: Machine Learning, Fall 2020**\n","# **Week 3, Worksheet 3**\n","## **Bias and Variance**\n","\n","\n","- **Content Creators:** Kenneth Shinn\n","- **Content Reviewers:** Lyle Ungar, Michael Zhou\n","    \n","As we know from lecture, there is a tradeoff between a model's bias and variance. In this worksheet, we take a look at this trade off in action. \n","\n","<h5> Objectives: </h5> \n","<ul>\n","    <li>See how model complexity affects bias and variance. </li>\n","    <li>See how the number of observations affects variance</li>\n","</ul>\n","       "]},{"cell_type":"code","metadata":{"id":"IgJVwGn3YYw9","colab_type":"code","colab":{}},"source":["# run below\n","!pip install mlxtend\n","%pip install mlxtend --upgrade\n","\n","# if you're on a mac, and the above throws an error, uncomment and run below\n","# pip3 install mlxtend"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"NJeW20xiC8p1","colab":{}},"source":["import pandas as pd\n","import numpy as np\n","from random import uniform\n","import matplotlib.pyplot as plt\n","from sklearn.linear_model import LinearRegression\n","from sklearn.linear_model import Ridge\n","from sklearn.preprocessing import PolynomialFeatures\n","from sklearn.metrics import mean_squared_error\n","import statistics \n","from mlxtend.evaluate import bias_variance_decomp"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"Zjl3GKzMC8p9"},"source":["First let's construct a model and generate some data. The function below uses the simple model $y = x$, and adds an error term $\\epsilon \\sim N(0,25)$. Our goal over this worksheet is to see how adjusting model complexity and $n$ (the number of observations) changes model bias and variance. "]},{"cell_type":"code","metadata":{"colab_type":"code","id":"r_jt1xsvC8p-","colab":{}},"source":["# let's generate some data\n","# returns X_train, y_train, X_test, y_test \n","def generate_data(n_train, n_test):\n","    X_train = []\n","    y_train = []\n","    X_test = []\n","    y_test = []\n","    \n","    all_x_train = np.linspace(-10,10,n_train)\n","    all_x_test = np.linspace(-10, 10, n_test)\n","    \n","    for x in all_x_train:\n","        X_train.append(x)\n","        y_train.append(x + np.random.normal(0, 5^2))\n","\n","    for x in all_x_test:\n","        X_test.append(x)\n","        y_test.append(x + np.random.normal(0, 5^2))\n","        \n","    X_train = np.array(X_train).reshape(-1, 1)\n","    y_train = np.array(y_train).reshape(-1, 1)\n","    X_test = np.array(X_test).reshape(-1, 1)\n","    y_test = np.array(y_test).reshape(-1, 1)\n","    \n","    return X_train, y_train, X_test, y_test\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"aREkm9FQC8qD"},"source":["# Bias vs. Variance (Model Complexity)\n","\n","Here the model complexity will be determined by the polynominal degree. That is, a polynominal of degree 2 will assume the model to be $y = \\beta_0 + \\beta_1 x + \\beta_2 x^2$.\n","\n","The code below will pull some data from the above data generator and train a model of degree $d$. You can run the code multiple times to see how the model will change depending on the degree of the trained model. Now consider the following questions:\n","\n","* Do you expect a higher or lower degree to result in a greater change between instances?\n","* From our theory, would you expect the bias to go up or down as the degree increases? How about the variance? What about the MSE?"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"-cgkCfYGC8qF","colab":{}},"source":["# change the parameters here, d is the degree, n is the number of observations\n","d = 2\n","n = 10\n","# *****************************\n","\n","X_train, y_train, X_test, y_test = generate_data(n, n)\n","X_train_poly = PolynomialFeatures(degree=d).fit_transform(X_train)\n","X_test_poly = PolynomialFeatures(degree=d).fit_transform(X_test)\n","\n","lm = LinearRegression()\n","lm.fit(X_train_poly,y_train)\n","y_pred_poly = lm.predict(X_train_poly)\n","\n","sorted_zip = sorted(zip(X_train,y_pred_poly), key = lambda x: x[0])\n","X_train, y_pred_poly = zip(*sorted_zip)\n","\n","plt.figure(figsize=(20, 10))\n","plt.title('Degree ' + str(d) + ' model with training observations')\n","plt.scatter(X_train, y_train)\n","plt.plot(X_train, y_pred_poly, color = 'red')\n","plt.show()\n","mse, bias, var = bias_variance_decomp(lm, X_train_poly, y_train.flatten(), X_test_poly, y_test.flatten(), loss='mse', num_rounds=200, random_seed=1)\n","\n","print('MSE: ' + str(\"%.2f\" % mse))\n","print('Bias: ' + str(\"%.2f\" %bias))\n","print('Variance: ' + str(\"%.2f\" %var))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"lqfbbSRgC8qK"},"source":["## Plotting MSE, Bias, and Variance\n","\n","The below code and graph generate 15 models with varying degrees from 0 to 6 and finds the average bias, variance, and MSE for each model complexity (defined by degree). The purpose of this is to \"test\" the claims that:\n","\n","<ol>\n","    <li>Bias goes down</li>\n","    <li>Variance goes up</li>\n","    <li>MSE goes down, then up</li>\n","</ol>\n","\n","as the model complexity increases. "]},{"cell_type":"code","metadata":{"colab_type":"code","id":"A-ScqRvQC8qK","colab":{}},"source":["degrees = [0, 1, 2, 3, 4, 5, 6]\n","num_of_iterations = 15\n","n = 50"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"T2n85bqgC8qO","scrolled":false,"colab":{}},"source":["avg_mse_list = []\n","avg_bias_list = []\n","avg_variance_list = []\n","\n","for d in degrees:\n","    mse_list = []\n","    bias_list = []\n","    variance_list = []\n","    for i in range(num_of_iterations):\n","        X_train, y_train, _, _ = generate_data(n, n)\n","        X_train_poly = PolynomialFeatures(degree=d).fit_transform(X_train)\n","        X_test_poly = PolynomialFeatures(degree=d).fit_transform(X_test)\n","\n","        lm = LinearRegression()\n","        mse, bias, var = bias_variance_decomp(lm, X_train_poly, y_train.flatten(), X_test_poly, y_test.flatten(), loss='mse', num_rounds=1000, random_seed=1)\n","\n","        mse_list.append(mse)\n","        bias_list.append(bias)\n","        variance_list.append(var)\n","    \n","    avg_mse_list.append(statistics.mean(mse_list))\n","    avg_bias_list.append(statistics.mean(bias_list))\n","    avg_variance_list.append(statistics.mean(variance_list)) \n","\n","plt.figure(figsize=(20, 10))\n","plt.plot(avg_mse_list, color = 'red',label=\"MSE\")\n","plt.title('Graph of MSE Across Different Model Complexities')\n","plt.legend()\n","plt.show()\n","\n","plt.figure(figsize=(20, 10))\n","plt.plot(avg_bias_list, color = 'green', label= \"Bias\")\n","plt.title('Graph of Bias Across Different Model Complexities')\n","plt.legend()\n","plt.show()\n","\n","\n","plt.figure(figsize=(20, 10))\n","plt.plot(avg_variance_list, color = 'blue', label= \"Variance\")\n","plt.title('Graph of Variance Across Different Model Complexities')\n","plt.legend()\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"LGgKhTDpC8qR"},"source":["# Regularization and Bias/Variance\n","\n","Now, let's take a look at how regularization affects model bias and variance. Here, we will use Ridge regression (L2 regularization) and adjust the regularization strength $\\lambda$ to see how it affects bias and variance. \n","\n","Below, adjust $\\lambda$ and see how it affects the slope of the linear model, as well as the bias and variance. \n","\n","* Before running any code, as we increase $\\lambda$, what changes do you expect to see in the slope, bias, and variance?"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"Zk68gfqqC8qR","colab":{}},"source":["# change the parameters here, lmda is lambda, n is the number of observations\n","lmda = 1\n","n = 100\n","# *****************************\n","\n","X_train, y_train, X_test, y_test = generate_data(n, n)\n","\n","rlm = Ridge(lmda)\n","rlm.fit(X_train,y_train)\n","y_pred = rlm.predict(X_train)\n","\n","plt.figure(figsize=(20, 10))\n","plt.title('Ridge Regression Model with Lambda = ' + str(lmda) + ' (includes training observations)')\n","plt.scatter(X_train, y_train)\n","plt.plot(X_train, y_pred, color = 'red')\n","plt.show()\n","\n","mse, bias, var = bias_variance_decomp(rlm, X_train, y_train.flatten(), X_test, y_test.flatten(), loss='mse', num_rounds=200, random_seed=1)\n","\n","print('MSE: ' + str(\"%.2f\" %mse))\n","print('Bias: ' + str(\"%.2f\" %bias))\n","print('Variance: ' + str(\"%.2f\" %var))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"FDMhWLjxC8qV"},"source":["# Model Variance vs. Number of Observations\n","\n","Now, let's see how the variance of a model can decrease as the number of observations increases. For this example, we will can pin down the model and adjust the number of observations. From our theory, we know that the model variance will go down as we increase the number of observations. Let's see this in action below. "]},{"cell_type":"code","metadata":{"colab_type":"code","id":"ETk1VBurC8qV","colab":{}},"source":["# change the parameters here, d is the degree, n is the number of observations\n","d = 2\n","n = 5\n","# *****************************\n","\n","X_train, y_train, X_test, y_test = generate_data(n, n)\n","X_train_poly = PolynomialFeatures(degree=d).fit_transform(X_train)\n","X_test_poly = PolynomialFeatures(degree=d).fit_transform(X_test)\n","\n","lm = LinearRegression()\n","lm.fit(X_train_poly,y_train)\n","y_pred_poly = lm.predict(X_train_poly)\n","\n","sorted_zip = sorted(zip(X_train,y_pred_poly), key = lambda x: x[0])\n","X_train, y_pred_poly = zip(*sorted_zip)\n","\n","plt.figure(figsize=(20, 10))\n","plt.title('Degree ' + str(d) + ' model with ' + str(n) + ' training observations')\n","plt.scatter(X_train, y_train)\n","plt.plot(X_train, y_pred_poly, color = 'red')\n","plt.show()\n","mse, bias, var = bias_variance_decomp(lm, X_train_poly, y_train.flatten(), X_test_poly, y_test.flatten(), loss='mse', num_rounds=200, random_seed=1)\n","\n","print('MSE: ' + str(\"%.2f\" %mse))\n","print('Bias: ' + str(\"%.2f\" %bias))\n","print('Variance: ' + str(\"%.2f\" %var))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"2kn7hNGBC8qX"},"source":["## Visualizing the effects of $n$ on variance\n","\n","Below, we can visualize this decrease in variance by seeing how different instances of data can generate more similar models as the number of observations increase. Here, we keep the model degree constant (5), and vary the number of observations. Given the model complexity and the number of observations, data is drawn 5 times and the resulting trained models are graphed. "]},{"cell_type":"code","metadata":{"colab_type":"code","id":"lsbzj_4PC8qX","colab":{}},"source":["degrees = 5\n","num_of_iterations = 5\n","num_of_observations = [10, 100, 1000, 10000, 100000]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"AsoqQsQzC8qZ","scrolled":false,"colab":{}},"source":["for n in num_of_observations:\n","    plt.figure(figsize=(20, 10))\n","    for _ in range(num_of_iterations):\n","        X_train, y_train, X_test, y_test = generate_data(n, n)\n","        X_train_poly = PolynomialFeatures(degree=degrees).fit_transform(X_train)\n","\n","        lm = LinearRegression()\n","        lm.fit(X_train_poly,y_train)\n","        y_pred_poly = lm.predict(X_train_poly)\n","        sorted_zip = sorted(zip(X_train,y_pred_poly), key = lambda x: x[0])\n","        X_train, y_pred_poly = zip(*sorted_zip)\n","        plt.plot(X_train, y_pred_poly)\n","    \n","    plt.title(str(num_of_iterations) + ' Polynomial Regression Models of Degree ' + str(degrees) + ', Different Data Observation Instances From Same Data Generating Model: n = '+ str(n) )\n","    plt.show()\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"yjgBKN-XC8qb"},"source":["# Exercises\n","\n","* What do you see to be the relationship between the number of observations and variance of the models? What might happen if we decrease the model complexity? Do you think that it will \"converge\" faster or slower? Go ahead and try it out.\n","* What does this worksheet tell you about what needs to be considered when creating and using machine learning models? "]}]}